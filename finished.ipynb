{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet ultralytics\n",
        "!pip install --quiet vietocr"
      ],
      "metadata": {
        "id": "uL7K-kXCNLDI",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a360bcc1-0ed6-4daf-f781-c67184bb0585"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/1.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m133.9/133.9 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m948.0/948.0 kB\u001b[0m \u001b[31m58.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m112.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.0/303.0 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for gdown (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for prefetch-generator (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdf2image\n",
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eTeWD_XMJlAZ",
        "outputId": "0e91f659-c089-4b65-96d7-a4391120aad7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdf2image in /usr/local/lib/python3.12/dist-packages (1.17.0)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from pdf2image) (10.2.0)\n",
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install poppler-utils\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uU98dWxLGXO",
        "outputId": "27adb2e8-2e0a-406e-f490-b2bd88df40b6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  poppler-utils\n",
            "0 upgraded, 1 newly installed, 0 to remove and 35 not upgraded.\n",
            "Need to get 186 kB of archives.\n",
            "After this operation, 697 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 poppler-utils amd64 22.02.0-2ubuntu0.9 [186 kB]\n",
            "Fetched 186 kB in 1s (186 kB/s)\n",
            "Selecting previously unselected package poppler-utils.\n",
            "(Reading database ... 126371 files and directories currently installed.)\n",
            "Preparing to unpack .../poppler-utils_22.02.0-2ubuntu0.9_amd64.deb ...\n",
            "Unpacking poppler-utils (22.02.0-2ubuntu0.9) ...\n",
            "Setting up poppler-utils (22.02.0-2ubuntu0.9) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==2.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 305
        },
        "id": "S_m2L-ZhQtnq",
        "outputId": "4115c5a8-ddf4-4ce5-c2c8-5bfe2bf24cee",
        "collapsed": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==2.0\n",
            "  Downloading numpy-2.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/60.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-2.0.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.0 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m19.0/19.0 MB\u001b[0m \u001b[31m39.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "Successfully installed numpy-2.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              },
              "id": "bf76760f453740cb9a13882456414197"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chuyá»ƒn tá»« file pdf sang áº£nh vÃ  crop tiÃªu Ä‘á» thÃ nh áº£nh\n"
      ],
      "metadata": {
        "id": "BQgdDFoXazRa"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "o5d7lx0eTkAf",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import cv2\n",
        "import numpy as np\n",
        "from PyPDF2 import PdfReader,PdfWriter\n",
        "from pdf2image import convert_from_path"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Chuyá»ƒn cÃ¡c file pdf trong folder sang áº£nh"
      ],
      "metadata": {
        "id": "3BvJxx4gbQ-_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "collapsed": true,
        "id": "BMMATgq5ABUY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a41c45c4-77dd-437c-e9c2-81f98dcf294f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/lenh-01_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/pl-02_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/nqcp-01_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/nqcp-02_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/hp-01_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/qd-01_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/qd-02_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/qd-03_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/qd-04_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/qd-05_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/luat-01_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/luat-02_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/luat-03_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/luat-04_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/luat-05_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/pl-01_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/pl-03_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/pl-04_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/nd-01_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/nd-02_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/nd-03_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/nd-04_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/nd-05_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2010_622 + 623-01_2010_NQ-HÄTP_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2011_455 + 456-01_2011_NQ-HÄTP_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2022_11 + 12_01-2021-NQ-HÄTP._page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2013_53 + 54-01_CT-TTg_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2013_71 + 72-02_CT-TTg_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2014_403 + 404-06_CT-TTg_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2024_1113 + 1114_104-CÄ-TTg._page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2024_1249 + 1250_111-CÄ-TTg._page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2010_577 + 578-435_KH-BCÄXDNTM_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2023_1025 + 1026_59-KH-BCÄTKHNQT._page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2012_777 + 778-01_2012_NQLT_CP- BCHTWÄTN_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2023_1187 + 1188_01-2023-NQLT-CP-ÄCTUBTUÌ›MTTQVN._page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2017_481 + 482_403-2017-NQLT-UBTVQH14-CP-ÄCTUBTWMTTQVN._page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2013_723 + 724-14_2013_QCLN_BTP-BCA-TANDTC-VKSNDTC_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2018_937 + 938_2222-QCPH-BKHCN-BNNPTNT-BCT._page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2011_87 + 88-01_2011_TT-BCT_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2011_401+ 402-01_2011_TT-BNG_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2010_534 + 535-01_2010_TTLT-VKSNDTC-BCA-TANDTC_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2011_117 + 118-01_2011_TTLT-BCA-BKHCN_page1.jpg\n",
            "ðŸ–¼ï¸ Converted to image: /content/drive/MyDrive/sample_intern1/output/img_first_pages1/2011_225 + 226-01_2011_TTLT-BNV-BKHCN_page1.jpg\n"
          ]
        }
      ],
      "source": [
        "# Folder paths\n",
        "input_folder = '/content/drive/MyDrive/sample_intern1/sample'\n",
        "img_output_folder = '/content/drive/MyDrive/sample_intern1/output/img_first_pages'\n",
        "os.makedirs(img_output_folder, exist_ok=True)\n",
        "\n",
        "# Loop through PDF files\n",
        "for filename in os.listdir(input_folder):\n",
        "    if filename.lower().endswith('.pdf'):\n",
        "        file_path = os.path.join(input_folder, filename)\n",
        "        base_name = os.path.splitext(filename)[0]\n",
        "\n",
        "        # Convert first page to image only\n",
        "        images = convert_from_path(file_path, first_page=1, last_page=1)\n",
        "        img_output_path = os.path.join(img_output_folder, f'{base_name}_page1.jpg')\n",
        "        images[0].save(img_output_path, 'JPEG')\n",
        "        print(f'Converted to image: {img_output_path}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Train model YOLO Ä‘á»ƒ crop tiÃªu Ä‘á» cÃ¡c vÄƒn báº£n"
      ],
      "metadata": {
        "id": "eVRLS8FYbY_X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zYoMwzDxAtdV",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Load a model\n",
        "model = YOLO(\"yolov8n.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Train the model\n",
        "results = model.train(data=\"/content/drive/MyDrive/sample_intern1/dataset.yaml\", epochs=100, imgsz=640)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goQXmvSgOXOA",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Load pretrained model\n",
        "model = YOLO(\"/content/drive/MyDrive/sample_intern1/best.pt\")  # load a pretrained model (recommended for training)\n",
        "\n",
        "# Train the model\n",
        "results = model.train(data=\"/content/drive/MyDrive/sample_intern1/dataset.yaml\", epochs=100, imgsz=640)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Crop tá»«ng áº£nh trong folder"
      ],
      "metadata": {
        "id": "OBYfqlIVbsJD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c chá»©a áº£nh\n",
        "image_folder = \"/content/drive/MyDrive/sample_intern1/output/img_first_pages\"\n",
        "output_dir = \"/content/drive/MyDrive/sample_intern1/output/test/\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "# Táº£i mÃ´ hÃ¬nh YOLO\n",
        "model_crop = YOLO(\"/content/drive/MyDrive/sample_intern1/title_crop.pt\")\n",
        "\n",
        "# Biáº¿n Ä‘á»ƒ Ä‘Ã¡nh sá»‘ thá»© tá»± cÃ¡c áº£nh crop\n",
        "crop_count = 0\n",
        "\n",
        "# Láº·p qua táº¥t cáº£ cÃ¡c file áº£nh trong thÆ° má»¥c\n",
        "for filename in os.listdir(image_folder):\n",
        "    if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
        "        image_path = os.path.join(image_folder, filename)\n",
        "        image = cv2.imread(image_path)\n",
        "        results = model_crop(image)\n",
        "\n",
        "        # Duyá»‡t qua táº¥t cáº£ cÃ¡c káº¿t quáº£ dá»± Ä‘oÃ¡n\n",
        "        for result in results:\n",
        "            for box in result.boxes:\n",
        "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
        "                object_image = image[y1:y2, x1:x2]\n",
        "\n",
        "                output_filename = os.path.join(output_dir, f\"crop_{crop_count}.jpg\")\n",
        "                cv2.imwrite(output_filename, object_image)\n",
        "                crop_count += 1\n",
        "\n",
        "                print(f\"ÄÃ£ lÆ°u {output_filename}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1sva5A8kAr9o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c057097a-f4de-46c8-dba7-de6492fb952b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "0: 640x480 1 container, 8.1ms\n",
            "Speed: 3.2ms preprocess, 8.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_0.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.1ms\n",
            "Speed: 3.5ms preprocess, 6.1ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_1.jpg\n",
            "\n",
            "0: 640x480 1 container, 12.7ms\n",
            "Speed: 3.5ms preprocess, 12.7ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_2.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.0ms\n",
            "Speed: 3.6ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_3.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.4ms\n",
            "Speed: 3.8ms preprocess, 6.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_4.jpg\n",
            "\n",
            "0: 640x480 1 container, 5.9ms\n",
            "Speed: 3.6ms preprocess, 5.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_5.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.0ms\n",
            "Speed: 3.6ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_6.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.0ms\n",
            "Speed: 4.2ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_7.jpg\n",
            "\n",
            "0: 640x448 1 container, 7.4ms\n",
            "Speed: 4.4ms preprocess, 7.4ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 448)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_8.jpg\n",
            "\n",
            "0: 640x480 1 container, 7.0ms\n",
            "Speed: 3.2ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_9.jpg\n",
            "\n",
            "0: 640x480 1 container, 5.9ms\n",
            "Speed: 3.2ms preprocess, 5.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_10.jpg\n",
            "\n",
            "0: 640x480 1 container, 5.9ms\n",
            "Speed: 3.5ms preprocess, 5.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_11.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.1ms\n",
            "Speed: 4.3ms preprocess, 6.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_12.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.0ms\n",
            "Speed: 3.6ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_13.jpg\n",
            "\n",
            "0: 640x480 1 container, 8.9ms\n",
            "Speed: 4.3ms preprocess, 8.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_14.jpg\n",
            "\n",
            "0: 640x512 1 container, 6.7ms\n",
            "Speed: 3.5ms preprocess, 6.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 512)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_15.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.9ms\n",
            "Speed: 3.5ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_16.jpg\n",
            "\n",
            "0: 640x480 1 container, 5.9ms\n",
            "Speed: 3.9ms preprocess, 5.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_17.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.3ms\n",
            "Speed: 3.8ms preprocess, 6.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_18.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.4ms\n",
            "Speed: 4.2ms preprocess, 6.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_19.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.9ms\n",
            "Speed: 4.3ms preprocess, 6.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_20.jpg\n",
            "\n",
            "0: 640x480 1 container, 5.9ms\n",
            "Speed: 3.3ms preprocess, 5.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_21.jpg\n",
            "\n",
            "0: 640x480 1 container, 10.5ms\n",
            "Speed: 4.1ms preprocess, 10.5ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_22.jpg\n",
            "\n",
            "0: 640x480 1 container, 12.2ms\n",
            "Speed: 6.2ms preprocess, 12.2ms inference, 2.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_23.jpg\n",
            "\n",
            "0: 640x480 1 container, 11.3ms\n",
            "Speed: 4.5ms preprocess, 11.3ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_24.jpg\n",
            "\n",
            "0: 640x480 1 container, 32.3ms\n",
            "Speed: 11.6ms preprocess, 32.3ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_25.jpg\n",
            "\n",
            "0: 640x480 1 container, 16.8ms\n",
            "Speed: 4.7ms preprocess, 16.8ms inference, 3.0ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_26.jpg\n",
            "\n",
            "0: 640x480 1 container, 20.9ms\n",
            "Speed: 4.3ms preprocess, 20.9ms inference, 8.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_27.jpg\n",
            "\n",
            "0: 640x480 1 container, 38.6ms\n",
            "Speed: 5.5ms preprocess, 38.6ms inference, 3.3ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_28.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.3ms\n",
            "Speed: 3.6ms preprocess, 6.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_29.jpg\n",
            "\n",
            "0: 640x480 1 container, 5.9ms\n",
            "Speed: 3.6ms preprocess, 5.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_30.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.2ms\n",
            "Speed: 3.3ms preprocess, 6.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_31.jpg\n",
            "\n",
            "0: 640x480 1 container, 5.9ms\n",
            "Speed: 3.5ms preprocess, 5.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_32.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.3ms\n",
            "Speed: 3.7ms preprocess, 6.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_33.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.0ms\n",
            "Speed: 3.3ms preprocess, 6.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_34.jpg\n",
            "\n",
            "0: 640x480 1 container, 5.9ms\n",
            "Speed: 3.3ms preprocess, 5.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_35.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.2ms\n",
            "Speed: 3.3ms preprocess, 6.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_36.jpg\n",
            "\n",
            "0: 640x480 2 containers, 6.3ms\n",
            "Speed: 3.5ms preprocess, 6.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_37.jpg\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_38.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.3ms\n",
            "Speed: 3.5ms preprocess, 6.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_39.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.5ms\n",
            "Speed: 3.4ms preprocess, 6.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_40.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.3ms\n",
            "Speed: 4.1ms preprocess, 6.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_41.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.0ms\n",
            "Speed: 3.6ms preprocess, 6.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_42.jpg\n",
            "\n",
            "0: 640x480 1 container, 6.1ms\n",
            "Speed: 3.5ms preprocess, 6.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 480)\n",
            "ÄÃ£ lÆ°u /content/drive/MyDrive/sample_intern1/output/test/crop_43.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text detection (YOLO) vÃ  text recognition (vietocr)"
      ],
      "metadata": {
        "id": "oqXd1UpIcghq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "from vietocr.tool.predictor import Predictor\n",
        "from vietocr.tool.config import Cfg\n",
        "import os\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "\n",
        "# Load YOLO model (báº¡n thay Ä‘Æ°á»ng dáº«n model cá»§a báº¡n vÃ o Ä‘Ã¢y)\n",
        "yolo_model = YOLO(\"/content/drive/MyDrive/sample_intern1/text_det_best.pt\")\n",
        "\n",
        "# Khá»Ÿi táº¡o VietOCR cho recognition\n",
        "config = Cfg.load_config_from_name('vgg_transformer')\n",
        "config['cnn']['pretrained'] = False\n",
        "config['device'] = 'cuda:0'\n",
        "vietocr_predictor = Predictor(config)\n",
        "\n",
        "# ThÆ° má»¥c\n",
        "input_folder = '/content/drive/MyDrive/sample_intern1/output/test'\n",
        "result_folder = '/content/drive/MyDrive/sample_intern1/output/result_images'\n",
        "os.makedirs(result_folder, exist_ok=True)\n",
        "\n",
        "# Danh sÃ¡ch áº£nh\n",
        "image_extensions = ('.jpg', '.jpeg', '.png')\n",
        "image_files = [f for f in os.listdir(input_folder) if f.lower().endswith(image_extensions)]\n",
        "\n",
        "recognized_text_dict = {}\n",
        "\n",
        "for img_name in image_files:\n",
        "    img_path = os.path.join(input_folder, img_name)\n",
        "    image = cv2.imread(img_path)\n",
        "    if image is None:\n",
        "        print(f\"[SKIPPED] KhÃ´ng thá»ƒ Ä‘á»c áº£nh: {img_path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"[PROCESSING] {img_name}\")\n",
        "\n",
        "    # YOLO detect\n",
        "    results = yolo_model(img_path)  # Tráº£ vá» list results\n",
        "    recognized_text_list = []\n",
        "\n",
        "    for r in results:\n",
        "        boxes = r.boxes.xyxy.cpu().numpy().astype(int)  # [x_min, y_min, x_max, y_max]\n",
        "        for idx, (x_min, y_min, x_max, y_max) in enumerate(boxes):\n",
        "            # Cáº¯t áº£nh\n",
        "            cropped = image[y_min:y_max, x_min:x_max]\n",
        "            if cropped.size == 0:\n",
        "                print(f\"[SKIP CROP] Empty crop at {img_name} box {idx}\")\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                cropped_rgb = cv2.cvtColor(cropped, cv2.COLOR_BGR2RGB)\n",
        "                img_pil = Image.fromarray(cropped_rgb)\n",
        "                recognized_text = vietocr_predictor.predict(img_pil)\n",
        "            except Exception as e:\n",
        "                recognized_text = \"[ERROR]\"\n",
        "                print(f\"Recognition error: {e}\")\n",
        "\n",
        "            recognized_text_list.append(recognized_text)\n",
        "\n",
        "            # Váº½ box + text\n",
        "            cv2.rectangle(image, (x_min, y_min), (x_max, y_max), (0, 0, 255), 2)\n",
        "            cv2.putText(image, recognized_text, (x_min, max(0, y_min - 10)),\n",
        "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1)\n",
        "\n",
        "    # LÆ°u áº£nh káº¿t quáº£\n",
        "    result_img_path = os.path.join(result_folder, f\"result_{img_name}\")\n",
        "    cv2.imwrite(result_img_path, image)\n",
        "    recognized_text_dict[img_name] = recognized_text_list\n",
        "    print(f\"[DONE] {img_name} -> {result_img_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bxtjuPzGjL9y",
        "outputId": "a17d1494-8493-46be-90fc-698de413eb7c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n",
            "18533it [00:00, 27525.48it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PROCESSING] crop_0.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_0.jpg: 160x640 2 texts, 54.5ms\n",
            "Speed: 1.5ms preprocess, 54.5ms inference, 1.9ms postprocess per image at shape (1, 3, 160, 640)\n",
            "[DONE] crop_0.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_0.jpg\n",
            "[PROCESSING] crop_1.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_1.jpg: 128x640 2 texts, 55.5ms\n",
            "Speed: 1.8ms preprocess, 55.5ms inference, 1.5ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_1.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_1.jpg\n",
            "[PROCESSING] crop_2.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_2.jpg: 96x640 2 texts, 138.4ms\n",
            "Speed: 1.3ms preprocess, 138.4ms inference, 2.2ms postprocess per image at shape (1, 3, 96, 640)\n",
            "[DONE] crop_2.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_2.jpg\n",
            "[PROCESSING] crop_3.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_3.jpg: 128x640 4 texts, 17.3ms\n",
            "Speed: 1.7ms preprocess, 17.3ms inference, 2.4ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_3.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_3.jpg\n",
            "[PROCESSING] crop_4.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_4.jpg: 96x640 2 texts, 9.1ms\n",
            "Speed: 0.8ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 96, 640)\n",
            "[DONE] crop_4.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_4.jpg\n",
            "[PROCESSING] crop_5.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_5.jpg: 128x640 2 texts, 9.9ms\n",
            "Speed: 0.8ms preprocess, 9.9ms inference, 1.5ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_5.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_5.jpg\n",
            "[PROCESSING] crop_6.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_6.jpg: 128x640 2 texts, 8.2ms\n",
            "Speed: 1.0ms preprocess, 8.2ms inference, 1.6ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_6.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_6.jpg\n",
            "[PROCESSING] crop_7.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_7.jpg: 128x640 4 texts, 8.7ms\n",
            "Speed: 1.1ms preprocess, 8.7ms inference, 1.7ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_7.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_7.jpg\n",
            "[PROCESSING] crop_8.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_8.jpg: 128x640 4 texts, 13.0ms\n",
            "Speed: 1.7ms preprocess, 13.0ms inference, 1.5ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_8.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_8.jpg\n",
            "[PROCESSING] crop_9.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_9.jpg: 128x640 4 texts, 15.7ms\n",
            "Speed: 1.5ms preprocess, 15.7ms inference, 2.5ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_9.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_9.jpg\n",
            "[PROCESSING] crop_10.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_10.jpg: 96x640 2 texts, 11.3ms\n",
            "Speed: 1.0ms preprocess, 11.3ms inference, 1.6ms postprocess per image at shape (1, 3, 96, 640)\n",
            "[DONE] crop_10.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_10.jpg\n",
            "[PROCESSING] crop_11.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_11.jpg: 96x640 2 texts, 12.5ms\n",
            "Speed: 1.4ms preprocess, 12.5ms inference, 1.6ms postprocess per image at shape (1, 3, 96, 640)\n",
            "[DONE] crop_11.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_11.jpg\n",
            "[PROCESSING] crop_12.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_12.jpg: 192x640 2 texts, 63.2ms\n",
            "Speed: 1.2ms preprocess, 63.2ms inference, 1.5ms postprocess per image at shape (1, 3, 192, 640)\n",
            "[DONE] crop_12.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_12.jpg\n",
            "[PROCESSING] crop_13.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_13.jpg: 192x640 2 texts, 8.5ms\n",
            "Speed: 1.2ms preprocess, 8.5ms inference, 2.0ms postprocess per image at shape (1, 3, 192, 640)\n",
            "[DONE] crop_13.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_13.jpg\n",
            "[PROCESSING] crop_14.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_14.jpg: 96x640 2 texts, 8.9ms\n",
            "Speed: 0.8ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 96, 640)\n",
            "[DONE] crop_14.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_14.jpg\n",
            "[PROCESSING] crop_15.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_15.jpg: 160x640 2 texts, 9.8ms\n",
            "Speed: 1.1ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 160, 640)\n",
            "[DONE] crop_15.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_15.jpg\n",
            "[PROCESSING] crop_16.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_16.jpg: 160x640 2 texts, 15.0ms\n",
            "Speed: 2.4ms preprocess, 15.0ms inference, 2.2ms postprocess per image at shape (1, 3, 160, 640)\n",
            "[DONE] crop_16.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_16.jpg\n",
            "[PROCESSING] crop_17.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_17.jpg: 128x640 4 texts, 11.4ms\n",
            "Speed: 1.3ms preprocess, 11.4ms inference, 1.5ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_17.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_17.jpg\n",
            "[PROCESSING] crop_18.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_18.jpg: 128x640 3 texts, 13.1ms\n",
            "Speed: 1.5ms preprocess, 13.1ms inference, 1.9ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_18.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_18.jpg\n",
            "[PROCESSING] crop_19.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_19.jpg: 128x640 6 texts, 8.7ms\n",
            "Speed: 1.0ms preprocess, 8.7ms inference, 2.4ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_19.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_19.jpg\n",
            "[PROCESSING] crop_20.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_20.jpg: 128x640 4 texts, 8.6ms\n",
            "Speed: 1.1ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_20.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_20.jpg\n",
            "[PROCESSING] crop_21.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_21.jpg: 96x640 2 texts, 10.9ms\n",
            "Speed: 1.2ms preprocess, 10.9ms inference, 1.5ms postprocess per image at shape (1, 3, 96, 640)\n",
            "[DONE] crop_21.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_21.jpg\n",
            "[PROCESSING] crop_22.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_22.jpg: 160x640 3 texts, 9.3ms\n",
            "Speed: 1.1ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 160, 640)\n",
            "[DONE] crop_22.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_22.jpg\n",
            "[PROCESSING] crop_23.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_23.jpg: 128x640 3 texts, 12.7ms\n",
            "Speed: 1.6ms preprocess, 12.7ms inference, 2.1ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_23.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_23.jpg\n",
            "[PROCESSING] crop_24.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_24.jpg: 96x640 3 texts, 12.5ms\n",
            "Speed: 1.2ms preprocess, 12.5ms inference, 1.9ms postprocess per image at shape (1, 3, 96, 640)\n",
            "[DONE] crop_24.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_24.jpg\n",
            "[PROCESSING] crop_25.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_25.jpg: 96x640 4 texts, 8.6ms\n",
            "Speed: 0.8ms preprocess, 8.6ms inference, 1.7ms postprocess per image at shape (1, 3, 96, 640)\n",
            "[DONE] crop_25.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_25.jpg\n",
            "[PROCESSING] crop_26.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_26.jpg: 128x640 5 texts, 9.0ms\n",
            "Speed: 0.9ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_26.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_26.jpg\n",
            "[PROCESSING] crop_27.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_27.jpg: 160x640 5 texts, 10.1ms\n",
            "Speed: 1.1ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 160, 640)\n",
            "[DONE] crop_27.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_27.jpg\n",
            "[PROCESSING] crop_28.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_28.jpg: 96x640 2 texts, 10.1ms\n",
            "Speed: 0.8ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 96, 640)\n",
            "[DONE] crop_28.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_28.jpg\n",
            "[PROCESSING] crop_29.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_29.jpg: 128x640 3 texts, 14.1ms\n",
            "Speed: 1.0ms preprocess, 14.1ms inference, 2.0ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_29.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_29.jpg\n",
            "[PROCESSING] crop_30.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_30.jpg: 160x640 5 texts, 9.6ms\n",
            "Speed: 1.2ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 160, 640)\n",
            "[DONE] crop_30.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_30.jpg\n",
            "[PROCESSING] crop_31.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_31.jpg: 160x640 3 texts, 10.2ms\n",
            "Speed: 1.4ms preprocess, 10.2ms inference, 1.7ms postprocess per image at shape (1, 3, 160, 640)\n",
            "[DONE] crop_31.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_31.jpg\n",
            "[PROCESSING] crop_32.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_32.jpg: 96x640 3 texts, 12.1ms\n",
            "Speed: 0.9ms preprocess, 12.1ms inference, 1.5ms postprocess per image at shape (1, 3, 96, 640)\n",
            "[DONE] crop_32.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_32.jpg\n",
            "[PROCESSING] crop_33.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_33.jpg: 96x640 3 texts, 9.2ms\n",
            "Speed: 1.1ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 96, 640)\n",
            "[DONE] crop_33.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_33.jpg\n",
            "[PROCESSING] crop_34.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_34.jpg: 128x640 3 texts, 10.8ms\n",
            "Speed: 0.9ms preprocess, 10.8ms inference, 1.4ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_34.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_34.jpg\n",
            "[PROCESSING] crop_35.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_35.jpg: 128x640 5 texts, 7.9ms\n",
            "Speed: 0.8ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_35.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_35.jpg\n",
            "[PROCESSING] crop_36.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_36.jpg: 96x640 2 texts, 8.3ms\n",
            "Speed: 0.7ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 96, 640)\n",
            "[DONE] crop_36.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_36.jpg\n",
            "[PROCESSING] crop_37.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_37.jpg: 96x640 3 texts, 8.3ms\n",
            "Speed: 0.7ms preprocess, 8.3ms inference, 1.4ms postprocess per image at shape (1, 3, 96, 640)\n",
            "[DONE] crop_37.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_37.jpg\n",
            "[PROCESSING] crop_38.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_38.jpg: 160x640 7 texts, 9.5ms\n",
            "Speed: 1.2ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 160, 640)\n",
            "[DONE] crop_38.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_38.jpg\n",
            "[PROCESSING] crop_39.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_39.jpg: 192x640 7 texts, 8.9ms\n",
            "Speed: 1.2ms preprocess, 8.9ms inference, 1.4ms postprocess per image at shape (1, 3, 192, 640)\n",
            "[DONE] crop_39.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_39.jpg\n",
            "[PROCESSING] crop_40.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_40.jpg: 160x640 3 texts, 8.8ms\n",
            "Speed: 1.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 160, 640)\n",
            "[DONE] crop_40.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_40.jpg\n",
            "[PROCESSING] crop_41.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_41.jpg: 128x640 4 texts, 8.3ms\n",
            "Speed: 0.8ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 128, 640)\n",
            "[DONE] crop_41.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_41.jpg\n",
            "[PROCESSING] crop_42.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_42.jpg: 192x640 6 texts, 36.7ms\n",
            "Speed: 1.8ms preprocess, 36.7ms inference, 11.9ms postprocess per image at shape (1, 3, 192, 640)\n",
            "[DONE] crop_42.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_42.jpg\n",
            "[PROCESSING] crop_43.jpg\n",
            "\n",
            "image 1/1 /content/drive/MyDrive/sample_intern1/output/test/crop_43.jpg: 96x640 3 texts, 8.5ms\n",
            "Speed: 0.7ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 96, 640)\n",
            "[DONE] crop_43.jpg -> /content/drive/MyDrive/sample_intern1/output/result_images/result_crop_43.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recognized_text_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H3r4aXvVOXuN",
        "outputId": "dcca78a8-4d09-403b-f3fc-695623b46b6a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'crop_0.jpg': ['Lá»†NH', 'Vá» viá»‡c cÃ´ng bá»‘ PhÃ¡p lá»‡nh'],\n",
              " 'crop_1.jpg': ['PHÃP Lá»†NH', 'Æ¯U ÄÃƒI NGÆ¯á»œI CÃ“ CÃ”NG Vá»šI CÃCH Máº NG'],\n",
              " 'crop_2.jpg': ['NGHá»Š QUYáº¾T',\n",
              "  'Vá» Ä‘áº§u tÆ° má»Ÿ rá»™ng Cáº£ng hÃ ng khÃ´ng quá»‘c táº¿ PhÃº Quá»‘c'],\n",
              " 'crop_3.jpg': ['NGHá»Š QUYáº¾T',\n",
              "  'vá»›i má»¥c tiÃªu tÄƒng trÆ°á»Ÿng nÄƒm 2025 Ä‘áº¡t 8% trá»Ÿ lÃªn',\n",
              "  'kinh táº¿n xÃ£ há»™i vÃ  dá»± toÃ¡n ngÃ¢n sÃ¡ch nhÃ  nÆ°á»›c',\n",
              "  'Vá» nhiá»‡m vu, giáº£i phÃ¡p chá»§ yáº¿u thá»±c hiá»‡n káº¿ hoáº¡ch phÃ¡t triá»ƒn'],\n",
              " 'crop_4.jpg': ['HIáº¾N PHÃP', 'NÆ¯á»šC Cá»˜NG HÃ’A XÃƒ Há»˜I CHá»¦ NGHÄ¨A VIá»†T NAM'],\n",
              " 'crop_5.jpg': ['QUYáº¾T Äá»ŠNH', 'Vá» viá»‡c cho thÃ´i quá»‘c tá»‹ch viá»‡t nam'],\n",
              " 'crop_6.jpg': ['QUYáº¾T Äá»ŠNH', 'Quy Ä‘á»‹nh vá» cÆ¡ cáº¥u biá»ƒu giÃ¡ bÃ¡n láº» Ä‘iá»‡n'],\n",
              " 'crop_7.jpg': ['trÃªn Ä‘á»‹a bÃ n tá»‰nh',\n",
              "  'QUYáº¾T Äá»ŠNH',\n",
              "  'vÃ  sÃ¡t háº¡ch, cáº¥p chá»©ng chá»‰ hÃ nh nghá» hoáº¡t Ä‘á»™ng xÃ¢y dá»±ng háº¡ng I',\n",
              "  'PhÃ¢n cáº¥p tháº©m quyá»n cáº¥p chá»©ng chá»‰ nÄƒng lá»±c hoáº¡t Ä‘á»™ng xÃ¢y dá»±ng'],\n",
              " 'crop_8.jpg': ['QUYáº¾T Äá»ŠNH',\n",
              "  'quáº£n lÃ½ Ä‘áº¥t Ä‘ai trÃªn Ä‘á»‹a bÃ n thÃ nh phá»‘ Háº£i PhÃ²ng',\n",
              "  'Ban hÃ nh Quy Ä‘á»‹nh vá» cÆ¡ cháº¿ giÃ¡m sÃ¡t, Ä‘Ã¡nh giÃ¡, kiá»ƒm Ä‘á»‹nh cháº¥t lÆ°á»£ng vÃ  nháº­t',\n",
              "  'quy cháº¿ kiáº¿m tra, nghiá»‡m thu sáº£n pháº©m, dá»‹ch vá»¥ cÃ´ng trong lÄ©nh vá»±c'],\n",
              " 'crop_9.jpg': ['QUYáº¾T Äá»ŠNH',\n",
              "  'vÃ  chuyá»ƒn Ä‘á»•i sá»‘ tá»‰nh PhÃº Thá»',\n",
              "  'Quy Ä‘á»‹nh chá»©c nÄƒng, nhiá»‡m vá»¥, quyá»n háº¡n vÃ  cÆ¡ cáº¥u tá»• chá»©c',\n",
              "  'cá»§a Trung tÃ¢m Khoa há»c cÃ´ng nghá»‡, Ä‘á»‘i má»›i sÃ¡ng táº¡o'],\n",
              " 'crop_10.jpg': ['LUáº¬T', 'Tá»” CHá»¨C CHÃNH QUYá»€N Äá»ŠA PHÆ¯Æ NG'],\n",
              " 'crop_11.jpg': ['LUáº¬T', 'BAN HÃ€NH VÄ‚N Báº¢N QUY PHáº M PHÃP LUáº¬T'],\n",
              " 'crop_12.jpg': ['LUáº¬T', 'Tá»” CHá»¨C CHÃNH PHá»¦'],\n",
              " 'crop_13.jpg': ['LUáº¬T', 'Tá»” CHá»¨C CHÃNH PHá»¦'],\n",
              " 'crop_14.jpg': ['LUáº¬T',\n",
              "  'Sá»¬A Äá»ŽI, Bá»’ SÃšNG Má»˜T Sá» ÄIá»€U Cá»¦A LUáº¬T Tá»” CHá»¨C QUá»C Há»˜I'],\n",
              " 'crop_15.jpg': ['PHÃP Lá»†NH', 'QUáº¢N LÃ THá»Š TRÆ¯á»œNG'],\n",
              " 'crop_16.jpg': ['PHÃP Lá»†NH', 'Cáº¢NH SÃT MÃ”I TRÆ¯á»œNG'],\n",
              " 'crop_17.jpg': ['VÃ€ CÃ”NG Cá»¤ Há»’ TRá»¢',\n",
              "  'PHÃP Lá»†NH',\n",
              "  'Sá»¬A Äá»”I, Bá»’ SUNG Má»˜T Sá» ÄIá»€U Cá»¦A',\n",
              "  'PHÃP Lá»†NH QUáº¢N LÃ, Sá»¬ Dá»¤NG VÅ¨ KHÃ, Váº¬T LIá»†U Ná»”'],\n",
              " 'crop_18.jpg': ['NGHá»Š Äá»ŠNH',\n",
              "  'vÃ  thuá»™c lÄ©nh vá»±c bá»• trá»£ tÆ° phÃ¡p',\n",
              "  'Sá»­a Ä‘á»•i, bá»• sung má»™t sá»‘ Ä‘iá»u cá»§a cÃ¡c Nghá»‹ dá»‹nh'],\n",
              " 'crop_19.jpg': ['NGHá»Š Äá»ŠNH',\n",
              "  'Danh má»¥c hÃ ng hÃ³a vÃ  má»©c thuáº¿ tuyá»‡t Ä‘á»‘i, thuáº¿ há»—n há»£p,',\n",
              "  'thuáº¿ nháº­p kháº©u ngoÃ i háº¡n ngáº¡ch thuáº¿ quan',\n",
              "  'Thá»• sung Nghi Ä‘á»‹nh sá»‘ 26/2023/NÄ CP ngÃ y 31 thÃ¡ng 5 nÄƒm 2019',\n",
              "  'ChÃ­nh phá»§ vá» Biá»ƒu thuáº¿ xuáº¥t khÃ¢u, Biá»ƒu thuáº¿ nháº­p kháº©u Æ°u',\n",
              "  'Thuáº­n thuáº­n tháº¿ Sá»­a Ä‘Ã´i, há»“ sung Nghi Ä‘inh sá»‘ 26/2023/NÄ-CP ngÃ y 31 thÃ¡ng 5 nÄƒm 2023 cá»­a 1996'],\n",
              " 'crop_20.jpg': ['Ä‘Æ¡n vá»‹ sá»± nghiá»‡p cÃ´ng láº­p',\n",
              "  'NGHá»Š Äá»ŠNH',\n",
              "  '21 thÃ¡ng 6 nÄƒm 2021 cá»§a ChÃ­nh phá»§ quy Ä‘á»‹nh cÆ¡ cháº¿ tá»± chá»§ tÃ i chÃ­nh cá»§a',\n",
              "  'Sá»­a Ä‘á»•i, bá»“ sung má»™t sá»‘ Ä‘iá»u cá»§a Nghá»‹ Ä‘á»‹nh sá»‘ 60/2021/NÄ-CP ngÃ y'],\n",
              " 'crop_21.jpg': ['NGHá»Š Äá»ŠNH',\n",
              "  'Quy Ä‘á»‹nh CÆ¡ sá»Ÿ dá»¯ liá»‡u vá» phÃ²ng, chá»‘ng báº¡o lá»±c gia Ä‘Ã¬nh'],\n",
              " 'crop_22.jpg': ['NGHá»Š Äá»ŠNH',\n",
              "  'Quy Ä‘á»‹nh chá»©c nÄƒng, nhiá»‡m vá»¥, quyá»n háº¡n',\n",
              "  'vÃ  cÆ¡ cáº¥u tá»• chá»©c cá»§a Thanh tra ChÃ­nh phá»§'],\n",
              " 'crop_23.jpg': ['cá»§a Bá»™ luáº­t HÃ¬nh sá»±',\n",
              "  'NGHá»Š QUYáº¾T',\n",
              "  'HÆ°á»›ng dáº«n Ã¡p dá»¥ng má»™t sá»‘ quy Ä‘á»‹nh táº¡i Äiá»u 248 vÃ  Äiá»u 249'],\n",
              " 'crop_24.jpg': ['NGHá»Š QUYáº¾T',\n",
              "  'HÆ°á»›ng dáº«n thi hÃ nh má»™t sá»‘ quy Ä‘á»‹nh cá»§a Nghá»‹ quyáº¿t sá»‘ 56/2010/QH12',\n",
              "  'ngÃ y 24/11/2010 cá»§a Quá»‘c há»™i vá» viá»‡c thi hÃ nh Luáº­t Tá»‘ tá»¥ng hÃ nh chÃ­nh'],\n",
              " 'crop_25.jpg': ['NGHá»Š QUYáº¾T',\n",
              "  'HÆ°á»›ng dáº«n Ã¡p dá»¥ng Äiá»u 201 cá»§a Bá»™ luáº­t HÃ¬nh sá»±',\n",
              "  'viá»‡c xÃ©t xá»­ vá»¥ Ã¡n hÃ¬nh sá»± vá» tá»™i cho vay lÃ£i náº·ng trong giao dá»‹ch dÃ¢n sá»±',\n",
              "  'vÃ  viá»‡c xÃ©t xá»­ vá»¥ Ã¡n hÃ¬nh sá»± vá» tá»™i cho vay lÃ£i náº·ng trong giao chÃ­nh'],\n",
              " 'crop_26.jpg': ['CHá»ˆ THá»Š',\n",
              "  'Tra Lá»i kÃªu gá»i thi Ä‘ua Ã¡i quá»‘c - NgÃ y truyá»n thá»‘ng Thi Ä‘ua yÃªu nÆ°á»›c',\n",
              "  'Vá» viá»‡c tá»• chá»©c ká»³ niá»‡m,65 nÄƒm ngÃ y chá»§ tá»‹ch Há»“ ChÃ­ minh',\n",
              "  '(ngÃ y 11 thÃ¡ng 6 nÄƒm 1948 - ngÃ y 11 thÃ¡ng 6 nÄƒm 2013)',\n",
              "  '(ngÃ y 11 thÃ¡ng 6 nÄƒm 1948 - ngÃ y 11 thÃ¡ng 6)'],\n",
              " 'crop_27.jpg': ['CHá»ˆ THá»Š',\n",
              "  'cÃ´ng nghiá»‡p hÃ³a, hiá»‡n Ä‘áº¡i hÃ³a trong Ä‘iá»u kiá»‡n kinh táº¿ thá»‹ trÆ°á»ng',\n",
              "  'Tháº¿ viá»‡c triá»ƒn khai thá»±c hiá»‡n Káº¿t luáº­n sá»‘ 51-KL/TW ngÃ y 29 thÃ¡ng 10 nÄƒm 2012',\n",
              "  'Ä‘á»‹nh hÆ°á»›ng xÃ£ há»™i chá»§ nghÄ©a vÃ  há»™i nháº­p quá»‘c tháº¿ trong',\n",
              "  'Ä‘á»“i má»›i cÄƒn báº£n, toÃ n diá»‡n giÃ¡o dá»¥c vÃ  Ä‘Ã o tao, Ä‘Ã¡p á»©ng yÃªn'],\n",
              " 'crop_28.jpg': ['CHá»ˆ THá»Š',\n",
              "  'Vá» viá»‡c Ä‘áº©y máº¡nh tÃ¡i cÆ¡ cáº¥u doanh nghiá»‡p nhÃ  nÆ°á»›c'],\n",
              " 'crop_29.jpg': ['CÃ”NG ÄIá»†N',\n",
              "  'nhá»¯ng thÃ¡ng cuá»‘i nÄƒm 2024',\n",
              "  'Vá» viá»‡c Ä‘Ã´n Ä‘á»‘c Ä‘áº©y máº¡nh giáº£i ngÃ¢n vá»‘n Ä‘áº§u tÆ° cÃ´ng'],\n",
              " 'crop_30.jpg': ['CÃ”NG ÄIá»†N',\n",
              "  'Vá» viá»‡c táº­p trung thá»±c hiá»‡n cÃ¡c nhiá»‡m vá»¥, giáº£i phÃ¡p cáº¥p bÃ¡ch,',\n",
              "  'vá»›i ÄoÃ n Thanh tra láº§n thá»© 5 cá»§a Uy ban chÃ¢u Ã‚u',\n",
              "  'vÃ  khÃ´ng theo quy Ä‘á»‹nh (IUU), chuáº©n bá»‹ Ä‘Ã³n vÃ  lÃ m viá»‡c',\n",
              "  'trá»ng tÃ¢m chá»‘ng khai thÃ¡c háº£i sáº£n báº¥t há»£p phÃ¡p, khÃ´ng bÃ¡o cÃ¡o thá»i'],\n",
              " 'crop_31.jpg': ['Káº¾ HOáº CH',\n",
              "  'Triá»ƒn khai ChÆ°Æ¡ng trÃ¬nh má»¥c tiÃªu quá»‘c gia',\n",
              "  'xÃ¢y dá»±ng nÃ´ng thÃ´n má»›i giai Ä‘oáº¡n 2010 - 2020'],\n",
              " 'crop_32.jpg': ['Káº¾ HOáº CH',\n",
              "  'Nghá»‹ quyáº¿t sá»‘ 22-NQ/TW cá»§a Bá»™ ChÃ­nh trá»‹ vá» há»™i nháº­p quá»‘c táº¿?',\n",
              "  'Hoáº¡t Ä‘á»™ng cá»§a Ban Chá»‰ Ä‘áº¡o xÃ¢y dá»±ng Äá» Ã¡m Tá»•ng káº¿t 10 nÄƒm thá»±c hiá»‡n'],\n",
              " 'crop_33.jpg': ['NGHá»Š QUYáº¾T LIÃŠN Tá»ŠCH',\n",
              "  'Trung Æ°Æ¡ng ÄoÃ n Thanh niÃªn Cá»™ng sáº£n Há»“ ChÃ­ Minh',\n",
              "  'Ban hÃ nh Quy cháº¿ phá»‘i há»£p cÃ´ng tÃ¡c cá»§a ChÃ­nh phá»§ vÃ  Ban Cháº¥p hÃ nh'],\n",
              " 'crop_34.jpg': ['NGHá»Š QUYáº¾T LIÃŠN Tá»ŠCH',\n",
              "  'Vá» cÃ´ng tÃ¡c phá»‘i há»£p giá»¯a ChÃ­nh phá»§ vá»›i ÄoÃ n Chá»§ tá»‹ch',\n",
              "  'Uy ban Trung Æ°Æ¡ng máº·t tráº­n tá»‘ quá»‘c viá»‡t nam'],\n",
              " 'crop_35.jpg': ['NGHá»Š QUYáº¾T LIÃŠN Tá»ŠCH',\n",
              "  'cá»§a máº·t tráº­n Tá»• quá»‘c Viá»‡t Nam',\n",
              "  'Quy Ä‘á»‹nh chi tiáº¿t cÃ¡c hÃ¬nh thá»©c giÃ¡m sÃ¡t, pháº£n biá»‡n xÃ£ há»™i',\n",
              "  'Quy Ä‘á»‹nh chá»‹ tiáº¿t cÃ¡c hÃ¬nh thá»©c giÃ¡m sÃ¡t, pháº£ng',\n",
              "  'NGHI QUYáº¾T LIÃŠN Tá»ŠCH'],\n",
              " 'crop_36.jpg': ['QUY CHáº¾',\n",
              "  'Phá»‘i há»£p liÃªn ngÃ nh trong cÃ´ng tÃ¡c thi hÃ nh Ã¡n dÃ¢n sá»±'],\n",
              " 'crop_37.jpg': ['QUY CHáº¾',\n",
              "  'nÃ´ng thÃ´n vÃ  Bá»™ CÃ´ng ThÆ°Æ¡ng vá» xÃ¢y dá»±ng vÃ  quáº£n lÃ½ chá»‰ dáº«n Ä‘á»‹a lÃ½',\n",
              "  'Phá»‘i há»£p giá»¯a Bá»™ Khá»a há»c, vÃ  CÃ´ng nghá»‡, Bá»™ NÃ´ng nghiá»‡p vÃ  PhÃ¡t triá»ƒn'],\n",
              " 'crop_38.jpg': ['Bá»˜ CÃ”NG THÆ¯Æ NG',\n",
              "  'CONG BAO/Sá» 937 4938/NgÃ y 27-9-2018',\n",
              "  'Bá»˜ NÃ”NG NGHIá»†P VÃ€ PHÃT TRIá»‚N NÃ”NG THÃ”N',\n",
              "  'CÃ”NG BAO/Sá» 93',\n",
              "  '197So 9377? 938/NgÃ y 279-2018',\n",
              "  'CÃ”NG BAO/Sá» 9377? 938/NgÃ y',\n",
              "  '1938/Ngay 279-2018'],\n",
              " 'crop_39.jpg': ['THÃ”NG TÆ¯',\n",
              "  'CÃ´ng hÃ²a nhÃ¢n dÃ´n Truna Hoa',\n",
              "  'CÃ´ng thÆ°Æ¡ng thá»±c hiá»‡n Quy táº¯c Thá»§ tá»¥c cáº¥p vÃ  kiá»ƒm tra xuáº¥t xá»© sá»­a Ä‘á»•i',\n",
              "  'Sá»­a Ä‘á»•i, ThÃ´ng tm sá»‘ 36/2010/TT-BCT ngÃ y 15 thÃ¡ng 4L nÄƒm 2010 cá»§a Bá»™',\n",
              "  'vÃ  Quy táº¯c cá»¥ thá»ƒ máº·t hÃ ng theo há»‡ thá»‘ng hÃ i hÃ²a phiÃªn báº£n 2007 trong tháº¿ trung',\n",
              "  'Bá»‡nh táº¿ toÃ n diá»‡n giá»¯a Hiá»‡p há»™i cÃ¡c quá»‘c gia ÄÃ´ng Nam Ã vÃ  nÆ°á»›c',\n",
              "  'Thá»‹nh ThÆ°Æ¡ng máº¡i hÃ ng hÃ³a thuá»™c, Hiá»‡p, Ä‘á»‹nh Khung vá» Há»c Ná»™i'],\n",
              " 'crop_40.jpg': ['THÃ”NG TÆ¯',\n",
              "  'Quy Ä‘á»‹nh thá»§ tá»¥c cáº¥p Giáº¥y phÃ©p nháº­p cáº£nh thi hÃ i',\n",
              "  'hÃ i cá»‘t, tro cá»‘t vá» Viá»‡t Nam'],\n",
              " 'crop_41.jpg': ['THÃ”NG TÆ¯ LIÃŠN Tá»ŠCH',\n",
              "  'vá» tráº£ há»“ sÆ¡ Ä‘á»ƒ Ä‘iá»u tra bá»• sung',\n",
              "  'TrÆ°á»ng dÃ¢n thi hÃ nh cÃ¡c quy Ä‘á»‹nh cá»§a Bá»™ quáº­t Tá»• tá»¥ng hÃ¬nh sá»±',\n",
              "  'HÆ°á»›ng dáº«n thi hÃ nh cÃ¡c quy Ä‘á»‹nh cá»§a bá»™ luáº­t Tá»• tá»¥ng hÃ¬nh sá»±'],\n",
              " 'crop_42.jpg': ['cá»§a lá»±c lÆ°á»£ng Cáº£nh sÃ¡t nhÃ¢n dÃ¢n',\n",
              "  'THÃ”NG TÆ¯ LIÃŠN Tá»ŠCH',\n",
              "  'phÃ¡t hiá»‡n vÃ  xá»­ lÃ½ vi pháº¡m, phÃ¡p luáº­t vá» báº£o vá»‡ mÃ´i trÆ°á»ng',\n",
              "  'Quy Ä‘á»‹nh viá»‡c phá»‘i há»£p thá»±c hiá»‡n Quyáº¿t Ä‘á»‹nh sá»‘ 20/2009/QÄ-TTg',\n",
              "  'ngÃ y 10/02/2009 cá»§a Thá»§ tÆ°á»›ng ChÃ­nh phá»§ vá» quy dá»‹nh viá»‡c trang bá»‹',\n",
              "  'sá»­ dá»¥ng phÆ°Æ¡ng tiá»‡n, thiáº¿t bá»‹ ká»¹ thuáº­t trong cÃ´ng tÃ¡c phÃ²ng ngá»«a,'],\n",
              " 'crop_43.jpg': ['THÃ”NG TÆ¯ LIÃŠN Tá»ŠCH',\n",
              "  'chuyÃªn ngÃ nh kiá»ƒm soÃ¡t cháº¥t lÆ°á»£ng sáº£n pháº©m, hÃ ng hÃ³a',\n",
              "  'HÆ°á»›ng dáº«n viá»‡c chuyá»ƒn xáº¿p ngáº¡ch vÃ  xáº¿p lÆ°Æ¡ng Ä‘á»‘i vá»›i cÃ´ng chá»©c']}"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "van_ban_dict = {\n",
        "    'HIáº¾N PHÃP': [],\n",
        "    'Bá»˜ LUáº¬T': [],\n",
        "    'LUáº¬T': [],\n",
        "    'PHÃP Lá»†NH': [],\n",
        "    'Lá»†NH': [],\n",
        "    'NGHá»Š QUYáº¾T': [],\n",
        "    'NGHá»Š QUYáº¾T LIÃŠN Tá»ŠCH': [],\n",
        "    'NGHá»Š Äá»ŠNH': [],\n",
        "    'QUYáº¾T Äá»ŠNH': [],\n",
        "    'THÃ”NG TÆ¯': [],\n",
        "    'THÃ”NG TÆ¯ LIÃŠN Tá»ŠCH': [],\n",
        "    'BÃO CÃO': [],\n",
        "    'CHá»ˆ THá»Š': [],\n",
        "    'QUY CHáº¾': [],\n",
        "    'QUY Äá»ŠNH': [],\n",
        "    'THÃ”NG CÃO': [],\n",
        "    'THÃ”NG BÃO': [],\n",
        "    'HÆ¯á»šNG DáºªN': [],\n",
        "    'CHÆ¯Æ NG TRÃŒNH': [],\n",
        "    'Káº¾ HOáº CH': [],\n",
        "    'PHÆ¯Æ NG ÃN': [],\n",
        "    'Äá»€ ÃN': [],\n",
        "    'Dá»° ÃN': [],\n",
        "    'BIÃŠN Báº¢N': [],\n",
        "    'Tá»œ TRÃŒNH': [],\n",
        "    'Há»¢P Äá»’NG': [],\n",
        "    'CÃ”NG ÄIá»†N': [],\n",
        "    'Báº¢N GHI NHá»š': [],\n",
        "    'Báº¢N THá»ŽA THUáº¬N': [],\n",
        "    'GIáº¤Y á»¦Y QUYá»€N': [],\n",
        "    'GIáº¤Y Má»œI': [],\n",
        "    'GIáº¤Y GIá»šI THIá»†U': [],\n",
        "    'GIáº¤Y NGHá»ˆ PHÃ‰P': [],\n",
        "    'PHIáº¾U Gá»¬I': [],\n",
        "    'PHIáº¾U CHUYá»‚N': [],\n",
        "    'PHIáº¾U BÃO': [],\n",
        "    'Báº¢N SAO Y': [],\n",
        "    'Báº¢N TRÃCH SAO': [],\n",
        "    'Báº¢N SAO Lá»¤C': [],\n",
        "    'VÄ‚N Báº¢N KHÃC': []\n",
        "}\n",
        "\n",
        "\n",
        "for img_name, texts in recognized_text_dict.items():\n",
        "  classified = False\n",
        "  for text in texts:\n",
        "      if text.isupper():\n",
        "        if text in van_ban_dict:\n",
        "          van_ban_dict[text].append(img_name)\n",
        "          classified = True\n",
        "          break\n",
        "  if not classified:\n",
        "    van_ban_dict['VÄ‚N Báº¢N KHÃC'].append(img_name)"
      ],
      "metadata": {
        "id": "cF65k2a33xoy"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "van_ban_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6R0r33UC5aqT",
        "outputId": "be25f943-3730-4764-f24b-f3f5b1e179a9"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'HIáº¾N PHÃP': ['crop_4.jpg'],\n",
              " 'Bá»˜ LUáº¬T': [],\n",
              " 'LUáº¬T': ['crop_10.jpg',\n",
              "  'crop_11.jpg',\n",
              "  'crop_12.jpg',\n",
              "  'crop_13.jpg',\n",
              "  'crop_14.jpg'],\n",
              " 'PHÃP Lá»†NH': ['crop_1.jpg', 'crop_15.jpg', 'crop_16.jpg', 'crop_17.jpg'],\n",
              " 'Lá»†NH': ['crop_0.jpg'],\n",
              " 'NGHá»Š QUYáº¾T': ['crop_2.jpg',\n",
              "  'crop_3.jpg',\n",
              "  'crop_23.jpg',\n",
              "  'crop_24.jpg',\n",
              "  'crop_25.jpg'],\n",
              " 'NGHá»Š QUYáº¾T LIÃŠN Tá»ŠCH': ['crop_33.jpg', 'crop_34.jpg', 'crop_35.jpg'],\n",
              " 'NGHá»Š Äá»ŠNH': ['crop_18.jpg',\n",
              "  'crop_19.jpg',\n",
              "  'crop_20.jpg',\n",
              "  'crop_21.jpg',\n",
              "  'crop_22.jpg'],\n",
              " 'QUYáº¾T Äá»ŠNH': ['crop_5.jpg',\n",
              "  'crop_6.jpg',\n",
              "  'crop_7.jpg',\n",
              "  'crop_8.jpg',\n",
              "  'crop_9.jpg'],\n",
              " 'THÃ”NG TÆ¯': ['crop_39.jpg', 'crop_40.jpg'],\n",
              " 'THÃ”NG TÆ¯ LIÃŠN Tá»ŠCH': ['crop_41.jpg', 'crop_42.jpg', 'crop_43.jpg'],\n",
              " 'BÃO CÃO': [],\n",
              " 'CHá»ˆ THá»Š': ['crop_26.jpg', 'crop_27.jpg', 'crop_28.jpg'],\n",
              " 'QUY CHáº¾': ['crop_36.jpg', 'crop_37.jpg'],\n",
              " 'QUY Äá»ŠNH': [],\n",
              " 'THÃ”NG CÃO': [],\n",
              " 'THÃ”NG BÃO': [],\n",
              " 'HÆ¯á»šNG DáºªN': [],\n",
              " 'CHÆ¯Æ NG TRÃŒNH': [],\n",
              " 'Káº¾ HOáº CH': ['crop_31.jpg', 'crop_32.jpg'],\n",
              " 'PHÆ¯Æ NG ÃN': [],\n",
              " 'Äá»€ ÃN': [],\n",
              " 'Dá»° ÃN': [],\n",
              " 'BIÃŠN Báº¢N': [],\n",
              " 'Tá»œ TRÃŒNH': [],\n",
              " 'Há»¢P Äá»’NG': [],\n",
              " 'CÃ”NG ÄIá»†N': ['crop_29.jpg', 'crop_30.jpg'],\n",
              " 'Báº¢N GHI NHá»š': [],\n",
              " 'Báº¢N THá»ŽA THUáº¬N': [],\n",
              " 'GIáº¤Y á»¦Y QUYá»€N': [],\n",
              " 'GIáº¤Y Má»œI': [],\n",
              " 'GIáº¤Y GIá»šI THIá»†U': [],\n",
              " 'GIáº¤Y NGHá»ˆ PHÃ‰P': [],\n",
              " 'PHIáº¾U Gá»¬I': [],\n",
              " 'PHIáº¾U CHUYá»‚N': [],\n",
              " 'PHIáº¾U BÃO': [],\n",
              " 'Báº¢N SAO Y': [],\n",
              " 'Báº¢N TRÃCH SAO': [],\n",
              " 'Báº¢N SAO Lá»¤C': [],\n",
              " 'VÄ‚N Báº¢N KHÃC': ['crop_38.jpg']}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}